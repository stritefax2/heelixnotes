use crate::configuration::state::ServiceAccess;
use crate::repository::settings_repository::get_setting;
use async_openai::{
    config::OpenAIConfig,
    types::{
        ChatCompletionRequestSystemMessageArgs, ChatCompletionRequestUserMessageArgs,
        ChatCompletionRequestMessage, CreateChatCompletionRequestArgs,
    },
    Client as OpenAIClient,
};
use log::{debug, error, info};
use reqwest::Client;
use serde::{Deserialize, Serialize};
use std::time::Duration;

const CLEANUP_SYSTEM_PROMPT: &str = r#"You are a document cleanup assistant. Take the following raw text and produce a clean, well-formatted markdown document. Your job is to make the content presentable and professional:

- Fix grammar, spelling, and punctuation errors
- Add proper structure with markdown headings (#, ##, ###) where appropriate
- Organize content into clear paragraphs
- Use bullet points or numbered lists where the content naturally fits a list format
- Use **bold** and *italic* for emphasis where it improves readability
- Use blockquotes (>) for quotes or callouts if applicable
- Use code blocks for any code or technical content
- Preserve the original meaning â€” do not add or remove information
- Make it look polished and easy to read

Return ONLY the cleaned markdown. No explanations, no preamble, no wrapping in code fences."#;

// Claude types
#[derive(Serialize)]
struct ClaudeRequest {
    model: String,
    max_tokens: usize,
    messages: Vec<ClaudeMessage>,
    system: String,
    stream: bool,
}

#[derive(Serialize, Deserialize)]
struct ClaudeMessage {
    role: String,
    content: String,
}

#[derive(Deserialize)]
struct ClaudeResponse {
    content: Vec<ClaudeContent>,
}

#[derive(Deserialize)]
struct ClaudeContent {
    text: String,
}

// Gemini types
#[derive(Serialize)]
struct GeminiRequest {
    contents: Vec<GeminiContent>,
    generation_config: GeminiGenerationConfig,
}

#[derive(Serialize)]
struct GeminiContent {
    role: String,
    parts: Vec<GeminiPart>,
}

#[derive(Serialize)]
struct GeminiPart {
    text: String,
}

#[derive(Serialize)]
struct GeminiGenerationConfig {
    max_output_tokens: usize,
}

#[derive(Deserialize)]
struct GeminiResponse {
    candidates: Vec<GeminiCandidate>,
}

#[derive(Deserialize)]
struct GeminiCandidate {
    content: GeminiCandidateContent,
}

#[derive(Deserialize)]
struct GeminiCandidateContent {
    parts: Vec<GeminiCandidatePart>,
}

#[derive(Deserialize)]
struct GeminiCandidatePart {
    text: String,
}

// Ollama types
#[derive(Serialize)]
struct OllamaRequest {
    model: String,
    messages: Vec<OllamaMessage>,
    stream: bool,
}

#[derive(Serialize, Deserialize)]
struct OllamaMessage {
    role: String,
    content: String,
}

#[derive(Deserialize)]
struct OllamaResponse {
    message: OllamaMessage,
}

const ANTHROPIC_URL: &str = "https://api.anthropic.com/v1/messages";
const GEMINI_URL: &str = "https://generativelanguage.googleapis.com/v1/models/gemini-2.0-flash:generateContent";

#[tauri::command]
pub async fn clean_up_document_with_llm(
    app_handle: tauri::AppHandle,
    plain_text: String,
    provider: String,
    model_id: Option<String>,
) -> Result<String, String> {
    info!("Cleaning up document with provider: {}, model: {:?}", provider, model_id);

    if plain_text.trim().is_empty() {
        return Err("Document is empty, nothing to clean up.".to_string());
    }

    match provider.as_str() {
        "claude" => clean_up_with_claude(&app_handle, &plain_text, model_id).await,
        "openai" => clean_up_with_openai(&app_handle, &plain_text, model_id).await,
        "gemini" => clean_up_with_gemini(&app_handle, &plain_text, model_id).await,
        "local" => clean_up_with_local(&app_handle, &plain_text, model_id).await,
        _ => Err(format!("Unknown provider: {}", provider)),
    }
}

async fn clean_up_with_claude(
    app_handle: &tauri::AppHandle,
    plain_text: &str,
    model_id: Option<String>,
) -> Result<String, String> {
    let setting = app_handle.db(|db| get_setting(db, "api_key_claude").expect("Failed on api_key_claude"));

    if setting.setting_value.is_empty() {
        return Err("Claude API key is not configured. Please set it in Settings.".to_string());
    }

    let client = Client::builder()
        .timeout(Duration::from_secs(180))
        .build()
        .map_err(|e| format!("Failed to create client: {}", e))?;

    let model_to_use = match model_id.as_deref() {
        Some("claude-haiku-4-5") => "claude-haiku-4-5",
        Some("claude-3-5-sonnet-20241022") => "claude-3-5-sonnet-20241022",
        _ => "claude-sonnet-4-5",
    };

    let request_body = ClaudeRequest {
        model: model_to_use.to_string(),
        max_tokens: 8192,
        messages: vec![ClaudeMessage {
            role: "user".to_string(),
            content: plain_text.to_string(),
        }],
        system: CLEANUP_SYSTEM_PROMPT.to_string(),
        stream: false,
    };

    let response = client
        .post(ANTHROPIC_URL)
        .header("Content-Type", "application/json")
        .header("x-api-key", &setting.setting_value)
        .header("anthropic-version", "2023-06-01")
        .json(&request_body)
        .send()
        .await
        .map_err(|e| format!("Request to Claude API failed: {}", e))?;

    if response.status().is_success() {
        let response_body: ClaudeResponse = response
            .json()
            .await
            .map_err(|e| format!("Failed to parse Claude response: {}", e))?;
        let cleaned = response_body.content.first()
            .map(|c| c.text.trim().to_string())
            .unwrap_or_default();
        debug!("Claude cleanup complete, {} chars", cleaned.len());
        Ok(cleaned)
    } else {
        let error_message = response.text().await
            .map_err(|e| format!("Failed to read error: {}", e))?;
        error!("Claude API error: {}", error_message);
        Err(format!("Claude API error: {}", error_message))
    }
}

async fn clean_up_with_openai(
    app_handle: &tauri::AppHandle,
    plain_text: &str,
    model_id: Option<String>,
) -> Result<String, String> {
    let setting = app_handle.db(|db| get_setting(db, "api_key_open_ai").expect("Failed on api_key_open_ai"));

    if setting.setting_value.is_empty() {
        return Err("OpenAI API key is not configured. Please set it in Settings.".to_string());
    }

    let model_to_use = match model_id.as_deref() {
        Some(m) => m,
        _ => "gpt-5",
    };

    let messages: Vec<ChatCompletionRequestMessage> = vec![
        ChatCompletionRequestSystemMessageArgs::default()
            .content(CLEANUP_SYSTEM_PROMPT)
            .build()
            .unwrap()
            .into(),
        ChatCompletionRequestUserMessageArgs::default()
            .content(plain_text)
            .build()
            .unwrap()
            .into(),
    ];

    let request = CreateChatCompletionRequestArgs::default()
        .model(model_to_use)
        .messages(messages)
        .build()
        .map_err(|e| format!("Failed to build request: {}", e))?;

    let client = OpenAIClient::with_config(OpenAIConfig::new().with_api_key(&setting.setting_value));
    let response = client
        .chat()
        .create(request)
        .await
        .map_err(|e| format!("OpenAI API request failed: {}", e))?;

    let cleaned = response.choices.first()
        .and_then(|c| c.message.content.as_ref())
        .map(|s| s.trim().to_string())
        .unwrap_or_default();

    debug!("OpenAI cleanup complete, {} chars", cleaned.len());
    Ok(cleaned)
}

async fn clean_up_with_gemini(
    app_handle: &tauri::AppHandle,
    plain_text: &str,
    model_id: Option<String>,
) -> Result<String, String> {
    let setting = app_handle.db(|db| get_setting(db, "api_key_gemini").expect("Failed on api_key_gemini"));

    if setting.setting_value.is_empty() {
        return Err("Gemini API key is not configured. Please set it in Settings.".to_string());
    }

    let client = Client::builder()
        .timeout(Duration::from_secs(180))
        .build()
        .map_err(|e| format!("Failed to create client: {}", e))?;

    let contents = vec![GeminiContent {
        role: "user".to_string(),
        parts: vec![GeminiPart {
            text: format!("{}\n\n{}", CLEANUP_SYSTEM_PROMPT, plain_text),
        }],
    }];

    let api_url = format!("{}?key={}", GEMINI_URL, setting.setting_value);

    let request_body = GeminiRequest {
        contents,
        generation_config: GeminiGenerationConfig {
            max_output_tokens: 8192,
        },
    };

    let response = client
        .post(&api_url)
        .header("Content-Type", "application/json")
        .json(&request_body)
        .send()
        .await
        .map_err(|e| format!("Request to Gemini API failed: {}", e))?;

    if response.status().is_success() {
        let response_body: GeminiResponse = response
            .json()
            .await
            .map_err(|e| format!("Failed to parse Gemini response: {}", e))?;

        let cleaned = response_body.candidates.first()
            .and_then(|c| c.content.parts.first())
            .map(|p| p.text.trim().to_string())
            .unwrap_or_default();

        debug!("Gemini cleanup complete, {} chars", cleaned.len());
        Ok(cleaned)
    } else {
        let error_message = response.text().await
            .map_err(|e| format!("Failed to read error: {}", e))?;
        error!("Gemini API error: {}", error_message);
        Err(format!("Gemini API error: {}", error_message))
    }
}

async fn clean_up_with_local(
    app_handle: &tauri::AppHandle,
    plain_text: &str,
    model_id: Option<String>,
) -> Result<String, String> {
    let setting = app_handle.db(|db| get_setting(db, "local_model_url").expect("Failed on local_model_url"));
    let base_url = if setting.setting_value.is_empty() {
        "http://localhost:11434".to_string()
    } else {
        setting.setting_value
    };

    let client = Client::builder()
        .timeout(Duration::from_secs(300))
        .build()
        .map_err(|e| format!("Failed to create client: {}", e))?;

    let model_to_use = model_id.unwrap_or_else(|| "llama3.3:70b".to_string());

    let messages = vec![
        OllamaMessage {
            role: "system".to_string(),
            content: CLEANUP_SYSTEM_PROMPT.to_string(),
        },
        OllamaMessage {
            role: "user".to_string(),
            content: plain_text.to_string(),
        },
    ];

    let api_url = format!("{}/api/chat", base_url);

    let request_body = OllamaRequest {
        model: model_to_use,
        messages,
        stream: false,
    };

    let response = client
        .post(&api_url)
        .header("Content-Type", "application/json")
        .json(&request_body)
        .send()
        .await
        .map_err(|e| format!("Request to Ollama failed: {}. Make sure Ollama is running.", e))?;

    if response.status().is_success() {
        let response_body: OllamaResponse = response
            .json()
            .await
            .map_err(|e| format!("Failed to parse Ollama response: {}", e))?;

        let cleaned = response_body.message.content.trim().to_string();
        debug!("Ollama cleanup complete, {} chars", cleaned.len());
        Ok(cleaned)
    } else {
        let error_message = response.text().await
            .map_err(|e| format!("Failed to read error: {}", e))?;
        error!("Ollama error: {}", error_message);
        Err(format!("Ollama error: {}. Make sure Ollama is running and the model is downloaded.", error_message))
    }
}
